
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup

* NeurIPS 2019 
  This repository is a collection of the papers and my notes from attended talks
  and other various parts of NeurIPS 2019. I created this repository mostly for my
  own sake, but also because I found it painstaking to find the papers and slides
  for the various talks I wanted to go to. Included also are my personal notes that
  will be updated as the week goes on.
  
** Sunday
*** Habana Labs
     - [[./habana-labs.org][Notes]]
     - Talk was a brief showcase of the Habana Goya Inference Processor.
       If youre not familiar with this type of processor, they allow you
       to fine-tune the inference of your model by reducing the precision
       of your model without accuracy lost. This sentence liberally taken
       from the Nvidia website and summerizes the importance of reduced
       precision:
#+BEGIN_VERSE
  Reduced precision inference significantly reduces application
  latency, which is a requirement for many real-time services,
  auto and embedded applications.
#+END_VERSE
*** Facebook Hardware
    - [[./facebook-hardware.html][Notes]]
    - Very interesting talk on the Open Accelerator Intrastructure. Couldn't
      stay for the whole thing, but the jist of it is that the OAI stack brings
      an open source software feel to hardware. The stack aims to reduce the
      time to integration for AI systems. There is an impressive number of
      companies behind it including Habana. I couldl't stay for whole talk, but
      for more detail see link below.
    - [[https://www.opencompute.org/blog/new-open-accelerator-infrastructure-oai-sub-project-to-launch-within-the-ocp-server-project][OAI]]
*** ML in Finance
    - [[./ML-in-finance.html][Notes]]
    - Personal Note: Two Sigma folks were very nice and approachable.
    - Attend the end of the J.P. Morgan talk and the Two Sigma talk in
      this session. Both were very high level talks about ML in finance.
      J.P. Morgan covered the basics of RL and their application to
      financial data. Two Sigma's talk broke down different uses of ML
      for finding opportunies within the market. They provided an
      example of how break down an instagram photo into consumable
      data for CNNs(object-recognition) and LSTMs(sentiment analysis).
      The process of training forecasting models was also covered.
** Monday (tutorials)
*** Deep Learning with Bayesian Principles
     - [[./DL-with-Bayesian-Principles.org][Notes]]
     - Author, Mohammad Emtiyaz Khan, focused on the benefits of combining Bayesian Learning
       and Deep Learning approaches. He showed how to derive common DL optimizers like
       Adam and RMSProp from Bayesian principles. Khan stressed the importance of such
       Bayesian principles in lifelong learning. This talk was one of my favorites because
       Khan did a great job of distilling the benefits of Bayesian Learning into well explained
       equations. 
*** Efficent Processing of Deep Neural Network
    - [[./nn-processing.html][Notes]]
    - Vivenne Sze discussed the various processing methods available and being researched
      for AI computation. Specifically, Sze drilled home the impact of reads from DRAM in
      training. This talk was quite dense and covered many aspects of AI processing from
      chip specifics to co-design and Neural Architecture Search (NAS).
*** Reinforcement Learning: Past, Present, and future Prospectives
     - [[./RL-overview.html][Notes]]
     - Katja Hofmann, a part of Microsoft Research, talked about Reinforcement Learning (Rl) from
       inception to standard practice. Hofmann outlined Deep Q-Learning and some improvements that
       have been made to the method such as Boostrapped DQNs from Osband et al. (2018). She then
       explained the Actor Critic model and shared a number of papers I am going to go read. 
** Tuesday
*** Logarithmic Regret for Online Control
    - [[https://i-am-karan-singh.github.io/docs/log-control/summary.pdf][slides]]
*** Legendre Memory Units: Continuous Time Representation in RNNs
    - [[https://docs.google.com/document/d/1uarYP9YqcKx2Qh7sHJJhrY87C-mVZbNTAVqRKVPDXmI/edit][supplementary material]]
*** Point Voxel CNN for Efficent 3D Deep Learning
    - [[https://hanlab.mit.edu/projects/pvcnn/][site]]
    - [[./Point-Voxel-3D-DL.pdf][paper]]
*** Neural Networks with Cheap Differential Operators
*** Sequential Neural Processes
*** Conditional Independance Testing Using GANs
    - [[https://neurips.cc/media/Slides/nips/2019/westballsa+b(10-10-05)-10-10-40-15681-conditional_ind.pdf][slides]]
*** Causal Confusion in Imitation Learning
    - [[https://sites.google.com/view/causal-confusion][site]]
    - [[./Causal-Confusion.pdf][paper]]
    - [[https://github.com/pimdh/causal-confusion][code]]
*** Reducing the Varience in Online Optimization by Transporting Past Gradients
    - [[./Implicit-Gradient-Transport.pdf][paper]]
*** SySCD: A System-Aware Parallel Coordinate Descent Algorithm
    - [[https://neurips.cc/media/Slides/nips/2019/westexhibitionhallb(10-16-10)-10-17-05-15802-syscd_a_system.pdf][slides]]
    - [[https://www.zurich.ibm.com/snapml/neurips19/SySCD-poster-neurips19.pdf][poster]]
*** Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies
    - [[https://neurips.cc/media/Slides/nips/2019/westballsa+b(10-16-10)-10-17-15-15692-tight_regret_bo.pdf][slides]]
*** Hindsight Credit Assignment
    - [[https://www.youtube.com/watch?v=2EnckznIV2o&feature=youtu.be][Video]]
    - [[https://neurips.cc/media/Slides/nips/2019/westballa+b(10-16-10)-10-17-20-15693-hindsight_credi.pdf][slides]]
*** Weight Agnostic Neural Networks
    - [[./weight-agnostic-nn.pdf][paper]]
    - [[https://weightagnostic.github.io/slides/wann_slides.pdf][Slides]]
    - [[https://weightagnostic.github.io/][site]]
    - Questions the importance of architechture in the learning process. Makes the
      comparison to Precocial species who have certain abilities provided at birth.
    - Shows that a neural architechture search can perform on multiple reinforcement
      learning and supervised learning tasks.
    - Based on NEAT
** Wednesday
*** Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations
    - [[https://vsitzmann.github.io/srns/video/][video]]
    - [[https://vsitzmann.github.io/srns/slides.pdf][slides]]
    - [[./Scene-Representation-Networks.pdf][paper]]
    - [[https://vsitzmann.github.io/][Author Github.io]]
*** Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG
    - [[https://neurips.cc/media/Slides/nips/2019/westballsa+b(11-10-05)-11-10-35-15700-principal_compo.pdf][slides]]
    - [[./PCA-SVRG.pdf][paper]]
*** PIDForest: Anomaly Detection via Partial Identification
    - [[https://neurips.cc/media/Slides/nips/2019/westballsa+b(11-10-05)-11-10-40-15701-pidforest_anom.pdf][slides]]
    - [[./pidforest.pdf][paper]]
*** Guided Similarity Seperation for Image Retrieval
    - [[./Guided-Similarity-Seperation.pdf][paper]]
*** CNAPs: Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes
    - [[./multi-task-class-CNAP.pdf][paper]]
    - [[https://neurips.cc/media/Slides/nips/2019/westballsa+b(11-15-50)-11-16-05-15704-fast_and_flexib.pdf][slides]]

*** Beyond Online Balanced Descent: An Optimal Algorithmfor Smoothed Online Convex Optimization
     - [[https://neurips.cc/media/Slides/nips/2019/westexhibitionhallb(11-15-50)-11-16-40-15821-beyond_online_b.pdf][slides]]
     - [[./Online-balanced-descent.pdf][paper]]
